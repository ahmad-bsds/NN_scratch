{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Deep Neural Network from Scratch Using NumPy"
      ],
      "metadata": {
        "id": "aI0nSKb4eZcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions\n",
        "\n",
        "**Sigmoid**: Useful for probabilities but may suffer from vanishing gradients.\n",
        "\n",
        "**ReLU**: Commonly used due to simplicity and efficiency."
      ],
      "metadata": {
        "id": "2jf9dGEse37l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QUfYZ2iVdOCT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "metadata": {
        "id": "Jtq5MrFdddYH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n"
      ],
      "metadata": {
        "id": "_V8rLFBPdda0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n"
      ],
      "metadata": {
        "id": "T25wHjbvdddM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "0RPP-sdWfK44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n"
      ],
      "metadata": {
        "id": "-47aEVTYddft"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_derivative(y_true, y_pred):\n",
        "    return -2 * (y_true - y_pred) / y_true.size"
      ],
      "metadata": {
        "id": "Al-8Gr4KddjD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "57C50r0ofNY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Class\n",
        "class DeepNeuralNetwork:\n",
        "    def __init__(self, layers, activation=\"relu\"):\n",
        "        self.layers = layers\n",
        "        self.activation = activation\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.weights.append(np.random.randn(layers[i], layers[i + 1]) * 0.1)\n",
        "            self.biases.append(np.zeros((1, layers[i + 1])))\n",
        "\n",
        "    def _activate(self, x):\n",
        "        return relu(x) if self.activation == \"relu\" else sigmoid(x)\n",
        "\n",
        "    def _activate_derivative(self, x):\n",
        "        return relu_derivative(x) if self.activation == \"relu\" else sigmoid_derivative(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "\n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            z = np.dot(activations[-1], w) + b\n",
        "            zs.append(z)\n",
        "            activation = self._activate(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        return activations, zs\n",
        "\n",
        "    def backward(self, activations, zs, y_true):\n",
        "        grad_weights = []\n",
        "        grad_biases = []\n",
        "\n",
        "        # Compute output layer error\n",
        "        delta = mse_derivative(y_true, activations[-1]) * self._activate_derivative(zs[-1])\n",
        "\n",
        "        # Backpropagate\n",
        "        for i in range(len(self.layers) - 2, -1, -1):\n",
        "            grad_weights.insert(0, np.dot(activations[i].T, delta))\n",
        "            grad_biases.insert(0, np.sum(delta, axis=0, keepdims=True))\n",
        "            if i != 0:\n",
        "                delta = np.dot(delta, self.weights[i].T) * self._activate_derivative(zs[i - 1])\n",
        "\n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "    def update_params(self, grad_weights, grad_biases, lr):\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= lr * grad_weights[i]\n",
        "            self.biases[i] -= lr * grad_biases[i]\n",
        "\n",
        "    def fit(self, x, y, epochs=1000, lr=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            activations, zs = self.forward(x)\n",
        "            loss = mean_squared_error(y, activations[-1])\n",
        "            grad_weights, grad_biases = self.backward(activations, zs, y)\n",
        "            self.update_params(grad_weights, grad_biases, lr)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, x):\n",
        "        activations, _ = self.forward(x)\n",
        "        return activations[-1]"
      ],
      "metadata": {
        "id": "hZByFiVaddnP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying"
      ],
      "metadata": {
        "id": "PZRX9NPRfRIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "x = np.random.rand(500, 2)\n",
        "y = (x[:, 0] + x[:, 1] > 1).astype(int).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "_Hp1U8vnddqE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize input data\n",
        "x = (x - x.mean(axis=0)) / x.std(axis=0)"
      ],
      "metadata": {
        "id": "cM9Vg8o-d986"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and train the model\n",
        "model = DeepNeuralNetwork(layers=[2, 8, 8, 1], activation=\"relu\")\n",
        "model.fit(x, y, epochs=1000, lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aNhkP-td_yG",
        "outputId": "e7d4431f-6428-42c3-96c4-f846d7cb130d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.4649\n",
            "Epoch 100, Loss: 0.2484\n",
            "Epoch 200, Loss: 0.2414\n",
            "Epoch 300, Loss: 0.2360\n",
            "Epoch 400, Loss: 0.2269\n",
            "Epoch 500, Loss: 0.2099\n",
            "Epoch 600, Loss: 0.1788\n",
            "Epoch 700, Loss: 0.1335\n",
            "Epoch 800, Loss: 0.0910\n",
            "Epoch 900, Loss: 0.0694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(x)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "accuracy = np.mean(predictions == y)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQzxDzmteBuV",
        "outputId": "7fcf378f-6bea-4e89-ee0b-46ef7839bb21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.97\n"
          ]
        }
      ]
    }
  ]
}